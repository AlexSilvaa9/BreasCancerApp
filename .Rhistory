# Iterar sobre cada pliegue interno para selección de modelo
indices_inner <- 1:nrow(datos_entrenamiento_outer)
resultados_auc_inner <- numeric(k_inner)
resultados_sensibilidad_inner <- numeric(k_inner)
resultados_accuracy_inner <- numeric(k_inner)
for (i_inner in 1:k_inner) {
filter_data <- filter_dependency(datos_entrenamiento_outer,"PCR",colnames(datos_entrenamiento_outer),0.05)
if ("PCR.1" %in% colnames(filter_data)) {
filter_data <- filter_data[, !colnames(filter_data) %in% "PCR.1", drop = FALSE]
}
###############################
#### Aqui el filtrado##########
###############################
#con poca variables solo unaa la vez o filtrado o wrapper
# Obtener los índices para el pliegue interno actual
test_indices_inner <- indices_inner[folds_inner==i_inner]  # Índices correspondientes al pliegue interno de test
train_indices_inner <- setdiff(1:nrow(filter_data), test_indices_inner)  # Índices correspondientes al conjunto de entrenamiento interno
# Dividir datos en entrenamiento y test para el pliegue interno
datos_entrenamiento_inner <- filter_data[train_indices_inner, ]
datos_validacion <- filter_data[test_indices_inner, ]
# Aplicar oversampling en el conjunto de entrenamiento interno
datos_entrenamiento_oversampled <- ROSE(PCR ~ .,
data = datos_entrenamiento_inner,
seed = 123,
p = 0.28)$data
# Inicializar variables para almacenar el mejor modelo y su métrica asociada
mejor_auc <- 0
mejor_modelo_auc <- NULL
# Establecer la fórmula base para el modelo logístico
formula_base <- formula(PCR ~ 1)  # Fórmula inicial sin predictoras
# Obtener nombres de variables predictoras
variables <- names(datos_entrenamiento_oversampled)[-which(names(datos_entrenamiento_oversampled) == "PCR")]
# Obtener todas las combinaciones posibles de variables predictoras
combinaciones_variables <- unlist(lapply(1:length(variables), function(x) combn(variables, x, simplify = FALSE)), recursive = FALSE)
# Iterar sobre todas las combinaciones de variables
for (combinacion in combinaciones_variables) {
# Construir la fórmula con las variables actuales
formula_actual <- as.formula(paste("PCR", "~", paste(combinacion, collapse = "+")))
# Ajustar el modelo logístico
modelo_actual <- glm(formula_actual, data = datos_entrenamiento_oversampled, family = binomial("logit"))
# Predecir y calcular el AUC en los datos de validación
predicciones_validacion <- predict(modelo_actual, newdata = datos_validacion, type = "response")
# Calcular el AUC
auc_actual <- pROC::roc(as.numeric(datos_validacion$PCR) - 1, predicciones_validacion)$auc
# Actualizar el mejor AUC y el mejor modelo si es necesario
if (auc_actual > mejor_auc) {
mejor_auc <- auc_actual
mejor_modelo_auc <- modelo_actual
}
}
# Realizar predicciones en el conjunto de entrenamiento externo para el AUC general
predicciones_auc[test_indices_inner] <- predict(mejor_modelo_auc, newdata = datos_validacion, type = "response")
predicciones_clasificadas <- ifelse(predicciones_auc[test_indices_inner] >= 0.5, "SI", "NO")
resultados_sensibilidad_inner[i_inner] <- sum(predicciones_clasificadas == "SI" & datos_validacion[["PCR"]] == "SI") / sum(datos_validacion[["PCR"]] == "SI")
resultados_accuracy_inner[i_inner] <- mean(predicciones_clasificadas == datos_validacion[["PCR"]])
targets_auc[test_indices_inner] <- as.numeric(datos_validacion$PCR) - 1
}
# Calcular el AUC final para este fold externo
auc_fold_externo <- pROC::roc(targets_auc, predicciones_auc)$auc
accuracy_fold_externo <- mean(resultados_accuracy_inner)
sensibilidad_fold_externo <- mean(resultados_sensibilidad_inner)
# Almacenar el AUC del fold externo actual
resultados_auc[i_outer] <- auc_fold_externo
resultados_accuracy[i_outer] <- accuracy_fold_externo
resultados_sensibilidad[i_outer] <- sensibilidad_fold_externo
# Predecir en los datos de test para este pliegue externo y almacenar predicciones
predicciones_test <- predict(mejor_modelo_auc, newdata = datos_test_outer, type = "response")
predicciones_totales[test_indices_outer, i_outer] <- predicciones_test
targets_totales[test_indices_outer] <- as.numeric(datos_test_outer$PCR) - 1
}
# Calcular AUC promedio
auc_promedio <- mean(resultados_auc)
accuracy_promedio <- mean(resultados_accuracy)
sensibilidad_promedio <- mean(resultados_sensibilidad)
# Crear tabla con todas las métricas
tabla_metricas <- data.frame(
Pliegue = 1:k_outer,
AUC = sapply(resultados_auc, round, digits = 3),
Accuracy = sapply(resultados_accuracy, round, digits = 3),
Recall = sapply(resultados_sensibilidad, round, digits = 3)
)
# Agregar fila con promedio de métricas
tabla_metricas <- rbind(tabla_metricas, c("Mean", round(auc_promedio,3), round(accuracy_promedio,3), round(sensibilidad_promedio,3)))
# Imprimir tabla formateada con kableExtra
kable(tabla_metricas, caption = "Cross Validation Metrics", align = "c") %>%
kable_styling(full_width = FALSE)
# Estructurar los datos en un formato largo
# Transformar la tabla a un formato largo
tabla_metricas_long <- pivot_longer(tabla_metricas, cols = c(AUC, Accuracy, Recall), names_to = "Metrica", values_to = "Valor")
# Crear el gráfico para AUC
auc_plot <- ggplot(tabla_metricas_long, aes(x = Pliegue, y = Valor, color = Metrica)) +
geom_point() +
geom_line(aes(group=Metrica)) +
labs(title = "Metric per Fold", x = "Outer Fold", y = "Value") +
theme_minimal() +
scale_color_manual(values = c("AUC" = "#7469B6", "Accuracy" = "#7AB2B2", "Recall" = "#3C5B6F"), name = "Metric")
print(auc_plot)
# Definir la fórmula para el modelo (sustituye "var_pred1 + var_pred2 + ..." con tus variables predictoras)
formula <- PCR ~ .
# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]
# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]
# Oversampling
proportion <- 0.28
datos_entrenamiento_oversampled <- ROSE(PCR ~ ., data = datos_entrenamiento_split, seed = 123, p = proportion)$data
# Inicializar variables para almacenar los resultados
mejor_auc <- 0
mejor_modelo <- NULL
# Definir combinaciones de hiperparámetros
params <- expand.grid(size = c(10, 15, 20), decay = c(0.01, 0.05, 0.1), maxit = c(500, 1000, 1500), skip = c(0, 1, 2))
# Iterar sobre las combinaciones de hiperparámetros
for (param in 1:nrow(params)) {
size <- params$size[param]
decay <- params$decay[param]
maxit <- params$maxit[param]
skip <- params$skip[param]
# Entrenar el modelo con la combinación de hiperparámetros actual
modelo <- nnet(formula, data = datos_entrenamiento_oversampled, size = size, decay = decay, maxit = maxit, skip = skip, trace = FALSE)
# Realizar predicciones en datos de validación
predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "raw")
# Calcular AUC para la combinación actual
roc_obj <- roc(ifelse(datos_validacion$PCR == "SI", 1, 0), ifelse(predicciones_validacion == "SI", 1, 0))
auc_actual <- auc(roc_obj)
# Actualizar el mejor modelo y su AUC si se encuentra un modelo mejor
if (auc_actual > mejor_auc) {
mejor_auc <- auc_actual
mejor_modelo <- modelo
}
}
# Imprimir el mejor modelo y su AUC
print(mejor_modelo)
print("Mejor AUC en datos de validación:")
print(mejor_auc)
# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "raw")
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$PCR,clases_predichas_test)
# Calcular la curva ROC y el AUC para el conjunto de test
prediccion_test <- prediction(predicciones_test, datos_test$PCR)
rendimiento_test <- performance(prediccion_test, "tpr", "fpr")
auc_test <- performance(prediccion_test, "auc")
cat("AUC :", auc_test@y.values[[1]], "\n")
# Agregar la curva ROC para el conjunto de test al gráfico
plot(rendimiento_test, main = "ROC Curve", col = "blue", lwd = 1)
# Agregar leyenda
legend("bottomright", legend = "Test", col =  "blue", lwd = 1)
# Imprimir tabla formateada con kableExtra
kable(tabla_metricas, caption = "Cross Validation Metrics", align = "c") %>%
kable_styling(full_width = FALSE)
# Estructurar los datos en un formato largo
# Transformar la tabla a un formato largo
tabla_metricas_long <- pivot_longer(tabla_metricas, cols = c(AUC, Accuracy, Recall), names_to = "Metrica", values_to = "Valor")
# Crear el gráfico para AUC
auc_plot <- ggplot(tabla_metricas_long, aes(x = Pliegue, y = Valor, color = Metrica)) +
geom_point() +
geom_line(aes(group=Metrica)) +
labs(title = "Metric per Fold", x = "Outer Fold", y = "Value") +
theme_minimal() +
scale_color_manual(values = c("AUC" = "#7469B6", "Accuracy" = "#7AB2B2", "Recall" = "#3C5B6F"), name = "Metric")
print(auc_plot)
# Imprimir tabla formateada con kableExtra
kable(tabla_metricas, caption = "Cross Validation Metrics", align = "c") %>%
kable_styling(full_width = FALSE)
# Estructurar los datos en un formato largo
# Transformar la tabla a un formato largo
tabla_metricas_long <- pivot_longer(tabla_metricas, cols = c(AUC, Accuracy, Recall), names_to = "Metrica", values_to = "Valor")
# Crear el gráfico para AUC
auc_plot <- ggplot(tabla_metricas_long, aes(x = Pliegue, y = Valor, color = Metrica)) +
geom_point() +
geom_line(aes(group=Metrica)) +
labs(title = "Metric per Fold", x = "Outer Fold", y = "Value") +
theme_minimal() +
scale_color_manual(values = c("AUC" = "#7469B6", "Accuracy" = "#7AB2B2", "Recall" = "#3C5B6F"), name = "Metric")
print(auc_plot)
# Imprimir tabla formateada con kableExtra
kable(tabla_metricas, caption = "Cross Validation Metrics", align = "c") %>%
kable_styling(full_width = FALSE)
# Estructurar los datos en un formato largo
# Transformar la tabla a un formato largo
tabla_metricas_long <- pivot_longer(tabla_metricas, cols = c(AUC, Accuracy, Recall), names_to = "Metrica", values_to = "Valor")
# Crear el gráfico para AUC
auc_plot <- ggplot(tabla_metricas_long, aes(x = Pliegue, y = Valor, color = Metrica)) +
geom_point() +
geom_line(aes(group=Metrica)) +
labs(title = "Metric per Fold", x = "Outer Fold", y = "Value") +
theme_minimal() +
scale_color_manual(values = c("AUC" = "#7469B6", "Accuracy" = "#7AB2B2", "Recall" = "#3C5B6F"), name = "Metric")
print(auc_plot)
# Imprimir tabla formateada con kableExtra
kable(tabla_metricas, caption = "Cross Validation Metrics", align = "c") %>%
kable_styling(full_width = FALSE)
# Estructurar los datos en un formato largo
# Transformar la tabla a un formato largo
tabla_metricas_long <- pivot_longer(tabla_metricas, cols = c(AUC, Accuracy, Recall), names_to = "Metrica", values_to = "Valor")
# Crear el gráfico para AUC
auc_plot <- ggplot(tabla_metricas_long, aes(x = Pliegue, y = Valor, color = Metrica)) +
geom_point() +
geom_line(aes(group=Metrica)) +
labs(title = "Metric per Fold", x = "Outer Fold", y = "Value") +
theme_minimal() +
scale_color_manual(values = c("AUC" = "#7469B6", "Accuracy" = "#7AB2B2", "Recall" = "#3C5B6F"), name = "Metric")
print(auc_plot)
#parameters to change:
proportion <- 0.28
sensitivity_weight <- 0.8
#valores distintos de 1 dan una sensibilidad malisima
# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]
# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]
datos_entrenamiento_oversampled <- ROSE(PCR ~ .,data=datos_entrenamiento_split,seed = 123,p =proportion)$data
# Cargar la biblioteca rpart para árboles de decisión
# Definir la fórmula para el modelo (sustituye "var_pred1 + var_pred2 + ..." con tus variables predictoras)
formula <- PCR ~ .
# Inicializar variables para almacenar los resultados
mejor_custom_metric <- 0
mejor_modelo <- NULL
# Definir combinaciones de hiperparámetros a probar
param_grid <- expand.grid(cp = c(0.01, 0.05, 0.1),
minsplit = c(5, 10, 20),
minbucket = c(5, 10, 20))
# Bucle para probar diferentes combinaciones de hiperparámetros
for (params in 1:nrow(param_grid)) {
cp <- param_grid$cp[params]
minsplit <- param_grid$minsplit[params]
minbucket <- param_grid$minbucket[params]
# Aquí va el código para entrenar y evaluar el modelo con los hiperparámetros cp, minsplit y minbucket
# Entrenar el modelo de árbol de decisión con la combinación de hiperparámetros actual
modelo <- rpart(formula, data = datos_entrenamiento_oversampled, control = rpart.control(cp = cp, minsplit = minsplit, minbucket = minbucket),method = "class")
# Realizar predicciones en datos de validación
predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "class")
# Calcular la sensibilidad y la precisión
TP <- sum(predicciones_validacion == "SI" & datos_validacion[["PCR"]] == "SI")
FN <- sum(predicciones_validacion == "NO" & datos_validacion[["PCR"]] == "SI")
FP <- sum(predicciones_validacion == "SI" & datos_validacion[["PCR"]] == "NO")
TN <- sum(predicciones_validacion == "NO" & datos_validacion[["PCR"]] == "NO")
recall_actual <- TP / (TP + FN)
accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
# Calcular el custom metric para la combinación actual
custom_metric_actual <- compute_custom_metric(recall_actual, accuracy_actual)
# Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
if (custom_metric_actual > mejor_custom_metric) {
mejor_custom_metric <- custom_metric_actual
mejor_modelo <- modelo
}
}
# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, probability = TRUE)
predicciones_test <- predicciones_test[, "SI"]  # Asumiendo que "SI" es la clase positiva
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$PCR,clases_predichas_test)
# Calcular la curva ROC y el AUC para el conjunto de test
prediccion_test <- prediction(predicciones_test, datos_test$PCR)
rendimiento_test <- performance(prediccion_test, "tpr", "fpr")
auc_test <- performance(prediccion_test, "auc")
cat("AUC :", auc_test@y.values[[1]], "\n")
# Agregar la curva ROC para el conjunto de test al gráfico
plot(rendimiento_test, main = "ROC Curve", col = "blue", lwd = 1)
# Agregar leyenda
legend("bottomright", legend = "Test", col =  "blue", lwd = 1)
#parameters to change:
proportion <- 0.28
sensitivity_weight <- 0.8
#valores distintos de 1 dan una sensibilidad malisima
# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]
# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]
datos_entrenamiento_oversampled <- ROSE(PCR ~ .,data=datos_entrenamiento_split,seed = 123,p =proportion)$data
# Definir la función para calcular el métrico personalizado
compute_custom_metric <- function(recall, accuracy, sensitivity_weight = 1) {
custom_metric <- (sensitivity_weight * recall) + ((1 - sensitivity_weight) * accuracy)
return(custom_metric)
}
# Definir la fórmula para el modelo (sustituye "var_pred1 + var_pred2 + ..." con tus variables predictoras)
formula <- PCR ~ .
# Inicializar variables para almacenar los resultados
mejor_custom_metric <- 0
mejor_modelo <- NULL
# Definir combinaciones de hiperparámetros a probar
param_grid <- expand.grid(usekernel = c(FALSE, TRUE),
laplace = c(FALSE, TRUE))
# Bucle para probar diferentes combinaciones de hiperparámetros
for (params in 1:nrow(param_grid)) {
usekernel <- param_grid$usekernel[params]
laplace <- param_grid$laplace[params]
# Aquí va el código para entrenar y evaluar el modelo con los hiperparámetros usekernel y laplace
# Entrenar el modelo Naive Bayes con la combinación de hiperparámetros actual
modelo <- naiveBayes(formula, data = datos_entrenamiento_oversampled, kernel = usekernel, laplace = laplace)
# Realizar predicciones en datos de validación
predicciones_validacion <- predict(modelo, newdata = datos_validacion)
# Calcular la sensibilidad y la precisión
TP <- sum(predicciones_validacion == "SI" & datos_validacion[["PCR"]] == "SI")
FN <- sum(predicciones_validacion == "NO" & datos_validacion[["PCR"]] == "SI")
FP <- sum(predicciones_validacion == "SI" & datos_validacion[["PCR"]] == "NO")
TN <- sum(predicciones_validacion == "NO" & datos_validacion[["PCR"]] == "NO")
recall_actual <- TP / (TP + FN)
accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
# Calcular el métrico personalizado para la combinación actual
custom_metric_actual <- compute_custom_metric(recall_actual, accuracy_actual)
# Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
if (custom_metric_actual > mejor_custom_metric) {
mejor_custom_metric <- custom_metric_actual
mejor_modelo <- modelo
}
}
# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "raw")
predicciones_test <- predicciones_test[, "SI"]  # Asumiendo que "SI" es la clase positiva
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$PCR,clases_predichas_test)
# Calcular la curva ROC y el AUC para el conjunto de test
prediccion_test <- prediction(predicciones_test, datos_test$PCR)
rendimiento_test <- performance(prediccion_test, "tpr", "fpr")
auc_test <- performance(prediccion_test, "auc")
cat("AUC :", auc_test@y.values[[1]], "\n")
# Agregar la curva ROC para el conjunto de test al gráfico
plot(rendimiento_test, main = "ROC Curve", col = "blue", lwd = 1)
# Agregar leyenda
legend("bottomright", legend = "Test", col =  "blue", lwd = 1)
# Definir la función para calcular el métrico personalizado
compute_custom_metric <- function(recall, accuracy, sensitivity_weight = 1) {
custom_metric <- (sensitivity_weight * recall) + ((1 - sensitivity_weight) * accuracy)
return(custom_metric)
}
# Parámetros a cambiar
proportion <- 0.28
sensitivity_weight <- 0.8
# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]
# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]
# Realizar sobremuestreo en los datos de entrenamiento
datos_entrenamiento_oversampled <- ROSE(PCR ~ ., data = datos_entrenamiento_split, seed = 123, p = proportion)$data
# Definir la función para calcular el métrico personalizado
compute_custom_metric <- function(recall, accuracy, sensitivity_weight = 1) {
custom_metric <- (sensitivity_weight * recall) + ((1 - sensitivity_weight) * accuracy)
return(custom_metric)
}
# Definir la fórmula para el modelo
formula <- PCR ~ .
# Inicializar variables para almacenar los resultados
mejor_custom_metric <- 0
mejor_modelo <- NULL
# Definir combinaciones de hiperparámetros a probar
param_grid <- expand.grid(mtry = c(2, 3, 4),
ntree = c(100, 200, 300),
nodesize = c(5, 10, 15))
# Bucle para probar diferentes combinaciones de hiperparámetros
for (params in 1:nrow(param_grid)) {
mtry <- param_grid$mtry[params]
ntree <- param_grid$ntree[params]
nodesize <- param_grid$nodesize[params]
# Entrenar el modelo Random Forest con la combinación de hiperparámetros actual
modelo <- randomForest(formula, data = datos_entrenamiento_oversampled,
mtry = mtry, ntree = ntree, nodesize = nodesize)
# Realizar predicciones en datos de validación
predicciones_validacion <- predict(modelo, newdata = datos_validacion, type = "response")
# Calcular la sensibilidad y la precisión
TP <- sum(predicciones_validacion == "SI" & datos_validacion[["PCR"]] == "SI")
FN <- sum(predicciones_validacion == "NO" & datos_validacion[["PCR"]] == "SI")
FP <- sum(predicciones_validacion == "SI" & datos_validacion[["PCR"]] == "NO")
TN <- sum(predicciones_validacion == "NO" & datos_validacion[["PCR"]] == "NO")
recall_actual <- TP / (TP + FN)
accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
# Calcular el métrico personalizado para la combinación actual
custom_metric_actual <- compute_custom_metric(recall_actual, accuracy_actual, sensitivity_weight)
# Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
if (custom_metric_actual > mejor_custom_metric) {
mejor_custom_metric <- custom_metric_actual
mejor_modelo <- modelo
}
}
# Evaluar el mejor modelo en el conjunto de prueba
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "response")
# Calcular las predicciones para el conjunto de test
predicciones_test <- predict(mejor_modelo, newdata = datos_test, type = "prob")
predicciones_test <- predicciones_test[, "SI"]  # Asumiendo que "SI" es la clase positiva
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$PCR,clases_predichas_test)
# Calcular la curva ROC y el AUC para el conjunto de test
prediccion_test <- prediction(predicciones_test, datos_test$PCR)
rendimiento_test <- performance(prediccion_test, "tpr", "fpr")
auc_test <- performance(prediccion_test, "auc")
cat("AUC :", auc_test@y.values[[1]], "\n")
# Agregar la curva ROC para el conjunto de test al gráfico
plot(rendimiento_test, main = "ROC Curve", col = "blue", lwd = 1)
# Agregar leyenda
legend("bottomright", legend = "Test", col =  "blue", lwd = 1)
# Ejecutar la parte del código que involucra la función knn
predicciones_test <- knn(train = encoded_data_entrenamiento,
test = encoded_data_test,
cl = datos_entrenamiento_oversampled$PCR,
k = mejor_k,
use.all = TRUE,
prob = TRUE)
encoded_data_entrenamiento
# Parámetros a cambiar
proportion <- 0.28
sensitivity_weight <- 0.8
# Dividir el conjunto de datos en entrenamiento y test
set.seed(123)  # Establecer una semilla para reproducibilidad
indices_entrenamiento <- sample(nrow(data), 0.75 * nrow(data))  # 75% para entrenamiento
datos_entrenamiento <- data[indices_entrenamiento, ]
datos_test <- data[-indices_entrenamiento, ]
# Dividir datos de entrenamiento en entrenamiento y validación
train_indices <- sample(nrow(datos_entrenamiento), 0.8 * nrow(datos_entrenamiento))  # 80% para entrenamiento
datos_entrenamiento_split <- datos_entrenamiento[train_indices, ]
datos_validacion <- datos_entrenamiento[-train_indices, ]
# Realizar sobremuestreo en los datos de entrenamiento
datos_entrenamiento_oversampled <- ROSE(PCR ~ ., data = datos_entrenamiento_split, seed = 123, p = proportion)$data
# Definir la función para calcular el métrico personalizado
compute_custom_metric <- function(recall, accuracy, sensitivity_weight = 1) {
custom_metric <- (sensitivity_weight * recall) + ((1 - sensitivity_weight) * accuracy)
return(custom_metric)
}
# Inicializar variables para almacenar los resultados
mejor_custom_metric <- 0
mejor_modelo <- NULL
# Codificación One-Hot Encoding para variables categóricas
encoded_data_entrenamiento <- as.data.frame(model.matrix(~.-1, datos_entrenamiento_oversampled[, -which(names(datos_entrenamiento_oversampled) == "PCR")]))
encoded_data_test <- as.data.frame(model.matrix(~.-1, datos_test[, -which(names(datos_test) == "PCR")]))
# Bucle para probar diferentes combinaciones de hiperparámetros
for (k in 1:30) {
# Realizar predicciones en datos de validación
predicciones_validacion <- knn(train = encoded_data_entrenamiento,
test = encoded_data_test,
cl = datos_entrenamiento_oversampled$PCR,
k = k,
use.all = TRUE)
# Calcular la sensibilidad y la precisión
TP <- sum(predicciones_validacion == "SI" & datos_validacion$PCR == "SI")
FN <- sum(predicciones_validacion == "NO" & datos_validacion$PCR == "SI")
FP <- sum(predicciones_validacion == "SI" & datos_validacion$PCR == "NO")
TN <- sum(predicciones_validacion == "NO" & datos_validacion$PCR == "NO")
recall_actual <- TP / (TP + FN)
accuracy_actual <- (TP + TN) / (TP + FN + FP + TN)
# Calcular el métrico personalizado para la combinación actual
custom_metric_actual <- compute_custom_metric(recall_actual, accuracy_actual, sensitivity_weight)
# Actualizar el mejor modelo y su rendimiento si se encuentra un modelo mejor
if (custom_metric_actual > mejor_custom_metric) {
mejor_custom_metric <- custom_metric_actual
mejor_k <- k
}
}
# Ejecutar la parte del código que involucra la función knn
predicciones_test <- knn(train = encoded_data_entrenamiento,
test = encoded_data_test,
cl = datos_entrenamiento_oversampled$PCR,
k = mejor_k,
use.all = TRUE)
# Calcular la sensibilidad y la precisión en el conjunto de prueba
TP <- sum(predicciones_test == "SI" & datos_test$PCR == "SI")
FN <- sum(predicciones_test == "NO" & datos_test$PCR == "SI")
FP <- sum(predicciones_test == "SI" & datos_test$PCR == "NO")
TN <- sum(predicciones_test == "NO" & datos_test$PCR == "NO")
recall_test <- TP / (TP + FN)
accuracy_test <- (TP + TN) / (TP + FN + FP + TN)
# Calcular el métrico personalizado en el conjunto de prueba
custom_metric_test <- compute_custom_metric(recall_test, accuracy_test, sensitivity_weight)
# Imprimir los resultados
cat("Recall (prueba):", recall_test, "\n")
cat("Accuracy (prueba):", accuracy_test, "\n")
cat("Métrico personalizado (prueba):", custom_metric_test, "\n")
encoded_data_entrenamiento
# Ejecutar la parte del código que involucra la función knn
predicciones_test <- knn(train = encoded_data_entrenamiento,
test = encoded_data_test,
cl = datos_entrenamiento_oversampled$PCR,
k = mejor_k,
use.all = TRUE,
prob = TRUE)
predicciones_test <- attr(predicciones_test, "prob")  # Asumiendo que "SI" es la clase positiva
# Calcular las predicciones para el conjunto de test
clases_predichas_test <- ifelse(predicciones_test >= 0.5, "SI", "NO")
mostrar_metricas(datos_test$PCR,clases_predichas_test)
shiny::runApp('~/Documents/UNI/Tercero/Segundo-cuatri/mineria/CancerApp')
rsconnect::setAccountInfo(name='g9bjvd-alex0silva', token='7A941A2B137F3716BD5C5BAF649212DE', secret='NOk7Yju38/FFoDp9fCUKf1QOO9ise6rgPnPvzCV7')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
library(rsconnect)
rsconnect::deployApp(.)
library(rsconnect)
rsconnect::deployApp(".")
getwd()
setwd("/home/alex/Documents/UNI/Tercero/Segundo-cuatri/mineria/CancerApp")
library(rsconnect)
rsconnect::deployApp(".")
shiny::runApp()
runApp()
runApp()
